{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summarization and Knowledge Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains code for all the methods to generte summaries and Knowledge graph using ChatGroq, into a json file for each text input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image is a sample json output after the summarization and Knowledge extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\Jsonoutput.jpg\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **NOTE** : Use langchain virtual env created!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary modules and library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Set\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create schema for the output from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for getting summary and relation entities from the chat groq\n",
    "class Relation(BaseModel):\n",
    "    source: str = Field(description=\"Source entity of the relation\")\n",
    "    target: str = Field(description=\"Target entity of the relation\")\n",
    "    relation: str = Field(description=\"Relation between the source and target entities\")\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    name: str = Field(description=\"Name of the entity\", alias=\"entity\")\n",
    "    type: str = Field(description=\"Type of the entity\")\n",
    "\n",
    "class BankruptcyLevel(BaseModel):\n",
    "    level: str = Field(description=\"Bankruptcy level of the company\", alias=\"lvl\")\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    summary: str = Field(description=\"Summary of the input text\")\n",
    "    bankruptcy_level: BankruptcyLevel = Field(description=\"Bankruptcy level of the company\")\n",
    "    entities: List[Entity] = Field(description=\"Entities extracted from the input text\")\n",
    "    relations: List[Relation] = Field(description=\"Relations extracted from the input text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityNormalizer:\n",
    "    def __init__(self):\n",
    "        self.company_suffixes = {\n",
    "            'limited', 'ltd', 'llc', 'inc', 'incorporated', 'corporation', 'corp',\n",
    "            'enterprise', 'enterprises', 'company', 'co', 'group', 'holdings',\n",
    "            'plc', 'ag', 'sa', 'nv', 'private', 'pvt'\n",
    "        }\n",
    "        self.known_entities = {}  # Maps normalized names to canonical names\n",
    "\n",
    "    def normalize_name(self, name: str) -> str:\n",
    "        name = name.lower()\n",
    "        name = ' '.join(name.split())\n",
    "        name = re.sub(r'[^\\w\\s&0-]', '', name)\n",
    "        words = name.split()\n",
    "        cleaned_words = [w for w in words if w not in self.company_suffixes]\n",
    "\n",
    "        if cleaned_words:\n",
    "            return ' '.join(cleaned_words)\n",
    "        return name\n",
    "    \n",
    "    def are_similar_entities(self, name1, name2, threshold = 0.95):\n",
    "        norm1 = self.normalize_name(name1)\n",
    "        norm2 = self.normalize_name(name2)\n",
    "\n",
    "        if norm1 == norm2:\n",
    "            return True\n",
    "        \n",
    "        similarity = SequenceMatcher(None, norm1, norm2).ratio()\n",
    "        return similarity >= threshold\n",
    "    \n",
    "    def get_canonical_name(self, name: str) -> str:\n",
    "        normalized = self.normalize_name(name)\n",
    "\n",
    "        for known_norm, canonical in self.known_entities.items():\n",
    "            if self.are_similar_entities(normalized, known_norm):\n",
    "                return canonical\n",
    "            \n",
    "        self.known_entities[normalized] = name\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary and Knowledge Extractor template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the prompt to build kg based on the relation other than person to company !\n",
    "# and also do chunking or just trim off the text to 6000 tokens and the daily token limit is 200000\n",
    "\n",
    "class SnKExtractor:\n",
    "    def __init__(self, api_key: str, model: str = \"llama-3.1-70b-versatile\"):\n",
    "        os.environ[\"GROQ_API_KEY\"] = api_key\n",
    "        self.llm = ChatGroq(temperature=0.5, model_name = model)\n",
    "        self.parser = PydanticOutputParser(pydantic_object=Summary)\n",
    "        self.prompt = self._create_prompt()\n",
    "        self.entity_normalizer = EntityNormalizer()\n",
    "        # print(self.parser.get_format_instructions())\n",
    "\n",
    "    def _create_prompt(self):\n",
    "        template = \"\"\"You are a financial Summarization and Knowledge Extraction System. Your task is to summarize and extract entities and relation from the given text and format them exactly according to the specified JSON structure. Only output the JSON structure, nothing else.\n",
    "\n",
    "Extract the following information from the given financial text of a company:\n",
    "\n",
    "Entities should be one of these types:\n",
    "1. COMPANY\n",
    "2. EVENT\n",
    "3. PRODUCT\n",
    "\n",
    "Relations should be one of these types:\n",
    "1. PARTICIPATES_IN (COMPANY -> EVENT)\n",
    "- Properties: Role (Organizer, Participant, Sponsor), Effect (-1 to 1)\n",
    "2. PRODUCES (COMPANY -> PRODUCT)\n",
    "- Properties: Production Volume, Production Start Date\n",
    "3. MENTIONS (EVENTS -> COMPANY/PRODUCT)\n",
    "- Properties: Sentiment (-1 to 1), Mention Count\n",
    "4. OWNS (COMPANY -> COMPANY)\n",
    "- Properties: Ownership Percentage, Acquisition Date\n",
    "5. COMPETES_WITH (COMPANY -> COMPANY)\n",
    "- Properties: Market Overlap Percentage\n",
    "6. HAD_NEGATIVE_IMPACT_ON (EVENT -> COMPANY)\n",
    "- Properties: Impact Level (0 to 1), Impact Type (Financial, Reputation, Legal), Reason\n",
    "7. HAD_POSITIVE_IMPACT_ON (EVENT -> COMPANY)\n",
    "- Properties: Impact Level (0 to 1), Impact Type (Financial, Reputation, Legal), Reason\n",
    "\n",
    "Summary should be the main content of the whole text provided.\n",
    "- Include the company's name and the bankruptcy level of the company.\n",
    "- Include the reason for the bankruptcy and the impact of the bankruptcy on the company.\n",
    "- Include the company's financial status and the company's future prospects.\n",
    "\n",
    "Bankruptcy Level should be between (-1 to 1), where -1 is the lowest level of bankruptcy and 1 is the highest level of bankruptcy.\n",
    "- Conclude corresponding to the sentiment of the company's financial status and future prospects.\n",
    "- If the company would not be bankrupt, the bankruptcy level should be -1. (Healthy Company)\n",
    "- If the comapny would be bankrupt, the bankruptcy level should be between 0.4 to 1. (Bankrupt Company)\n",
    "- If the company is in a critical situation, the bankruptcy level should be between 0 to 0.4. (Critical Company)\n",
    "\n",
    "Rules:\n",
    "1. Use full Company names consistently \n",
    "2. Do not repeat the contents\n",
    "3. Normalize company names (e.g., if \"Apple Inc.\" and \"Apple Corporation\" refer to the same company, use one consistent name)\n",
    "4. Output must be valid JSON format\n",
    "5. Use only the predefined entities and relations type\n",
    "6. The source and target have to be added as entities before forming a relation\n",
    "7. Focus mainly on the impacts of events on the company in relation extractions\n",
    "8. Impact level should be between 0 to 1 for both positive and negative impacts, 0 means no impact and 1 means the highest impact\n",
    "9. Bankruptcy level should be between -1 to 1\n",
    "10. Bankruptcy level should be concluded based on the sentiment of the company's financial status and future prospects\n",
    "11. Bankruptcy level should be between 0.4 to 1 if the sentiment of the text is negative and -1 to 0 if the sentiment of the text is positive\n",
    "12. Must create properties related to relations based on the given information in the text\n",
    "\n",
    "Input text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "        return ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    def _clean_llm_output(self, output: str) -> str:\n",
    "        try:\n",
    "            start = output.find('{')\n",
    "            end = output.rfind('}')\n",
    "            if start == -1 or end == -1:\n",
    "                raise ValueError(\"No JSON object found in output\")\n",
    "            json_str = output[start:end+1]\n",
    "            return json.dumps(json.load(json_str))\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse JSON from output: {e}\")\n",
    "        \n",
    "    def _disambiguate_entities(self, result: Summary) -> Summary:\n",
    "        name_mapping = {}\n",
    "        unique_entities = {}\n",
    "\n",
    "        for entity in result.entities:\n",
    "            canonical_name = self.entity_normalizer.get_canonical_name(entity.name)\n",
    "            name_mapping[entity.name] = canonical_name\n",
    "\n",
    "            if canonical_name not in unique_entities:\n",
    "                unique_entities[canonical_name] = entity.type\n",
    "        \n",
    "        new_entities = [\n",
    "            Entity(name = name, type = etype) for name, etype in unique_entities.items()\n",
    "        ]\n",
    "\n",
    "        new_relations = []\n",
    "        for _relation in result.relations:\n",
    "            new_relation = Relation(\n",
    "                source=name_mapping.get(_relation.source, _relation.source),\n",
    "                target=name_mapping.get(_relation.target, _relation.target),\n",
    "                relation=_relation.relation,\n",
    "                properties=_relation.properties\n",
    "            )\n",
    "            new_relations.append(new_relation)\n",
    "        \n",
    "        return Summary(summary=result.summary, entities=new_entities, relations=new_relations, bankruptcy_level=result.bankruptcy_level)\n",
    "    \n",
    "    def extract_summary_and_knowledge(self, financial_snippet: str, output_dir: str) -> Summary:\n",
    "        # Implement timer for each api call, can wait for a min after each request\n",
    "        # time consuming but can work! Max reties can be 5\n",
    "        retries = 0\n",
    "        wait_time = 0\n",
    "        while retries < 5:\n",
    "            try:\n",
    "                message = self.prompt.format_messages(\n",
    "                    text=financial_snippet,\n",
    "                    format_instructions=self.parser.get_format_instructions()\n",
    "                )\n",
    "                output = self.llm.invoke(message)\n",
    "\n",
    "                #-----------------------------------\n",
    "                # cleaned_output = self._clean_llm_output(output.model_dump()['content'])\n",
    "                # print(cleaned_output)\n",
    "                # result = self.parser.parse(cleaned_output)\n",
    "                # print(result)\n",
    "                # final_result = self._disambiguate_entities(result)\n",
    "                # print(final_result)\n",
    "                #-----------------------------------\n",
    "                # os.makedirs(output_dir, exist_ok=True)\n",
    "                # output_file = os.path.join(output_dir, \"extraction_result.json\")\n",
    "                \n",
    "                # with open(output_file, \"w\") as f:\n",
    "                    # json.dump(final_result.model_dump(), f, indent=4)\n",
    "\n",
    "                # return final_result\n",
    "                retries = 0\n",
    "                wait_time = 0\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                wait_time += 60\n",
    "                time.sleep(wait_time)\n",
    "                print(F\"Error during Summarization and Knowledge Extraction II: {str(e)}\")\n",
    "                return Summary(summary=\"Could not generate summary !\", entities=[], relations=[], bankruptcy_level=\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing on one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing it on one file.\n",
    "api_key = \"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\"\n",
    "summary_extractor = SnKExtractor(api_key)\n",
    "\n",
    "bankrupt_file = r'Dataset\\Final Dataset\\Bankrupt\\ADHUNIK_2015_MDA.txt'\n",
    "\n",
    "with open(bankrupt_file, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "try:\n",
    "    result = summary_extractor.extract_summary_and_knowledge(text, r\".\\output\\bankrupt\")\n",
    "    os.makedirs(r\".\\output\\bankrupt\", exist_ok=True)\n",
    "    output_file = os.path.join(r\".\\output\\bankrupt\", \"ADHUNIK_2015_MDA.txt.json\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(result.model_dump(), f, indent=4)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Summarization and Knowledge Extraction III: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the json output and saving it back to the file\n",
    "\n",
    "extracted_text_path = r'output\\bankrupt\\ADHUNIK_2015_MDA.txt.json'\n",
    "with open(extracted_text_path, 'r') as f:\n",
    "    extracted_text = json.load(f)\n",
    "    try:\n",
    "        start = extracted_text['content'].find('{')\n",
    "        end = extracted_text['content'].rfind('}')\n",
    "        if start == -1 or end == -1:\n",
    "            raise ValueError(\"No JSON object found in output\")\n",
    "        json_str = extracted_text['content'][start:end+1]\n",
    "        # print(json_str) # use it to see the json output\n",
    "        json_str = json.loads(json_str)\n",
    "        with open(r'output\\bankrupt\\ADHUNIK_2015_MDA.txt.json', 'w') as f:\n",
    "            json.dump(json_str, f, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse JSON from output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarization and Knowledge Extraction for all the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do chunking!\n",
    "```\n",
    "Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 9469, please reduce your message size and try again.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary and ner on all the files!\n",
    "api_keys = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\"]\n",
    "api_key = \"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\"\n",
    "summary_extractor = SnKExtractor(api_keys[2])\n",
    "\n",
    "bankrupt_files_path = r'Dataset\\Phase-II\\Bankrupt'\n",
    "healthy_files_path = r'Dataset\\Phase-II\\Healthy'\n",
    "\n",
    "bankrupt_files_output = r\".\\output\\bankrupt\"\n",
    "healthy_files_output = r\".\\output\\healthy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying tqdm\n",
    "total_files = len(os.listdir(bankrupt_files_path))\n",
    "import time\n",
    "\n",
    "for file in tqdm(os.listdir(bankrupt_files_path), total=total_files, desc=\"Processing files\"):\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:   2%|▏         | 5/201 [02:59<2:29:09, 45.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 7181, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:   3%|▎         | 6/201 [04:00<2:45:18, 50.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 9152, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:   3%|▎         | 7/201 [05:01<2:54:58, 54.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 9583, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:   4%|▍         | 8/201 [06:02<3:01:02, 56.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 12921, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:   6%|▋         | 13/201 [08:35<2:11:44, 42.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 13053, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  10%|█         | 21/201 [13:09<1:58:54, 39.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 7236, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  11%|█▏        | 23/201 [14:17<1:55:43, 39.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 15299, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  12%|█▏        | 24/201 [15:17<2:14:18, 45.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 6681, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  13%|█▎        | 26/201 [16:26<2:02:46, 42.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 8718, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  14%|█▍        | 28/201 [17:32<1:55:26, 40.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 8422, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  14%|█▍        | 29/201 [18:33<2:12:37, 46.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 8712, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  15%|█▍        | 30/201 [19:34<2:24:25, 50.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 9368, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  15%|█▌        | 31/201 [20:35<2:32:19, 53.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 9813, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  16%|█▌        | 32/201 [21:36<2:37:41, 55.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 9810, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  16%|█▋        | 33/201 [22:37<2:40:51, 57.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 6777, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  17%|█▋        | 34/201 [23:38<2:42:44, 58.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 6469, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  24%|██▍       | 48/201 [30:43<1:54:44, 44.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 7232, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  24%|██▍       | 49/201 [31:44<2:06:11, 49.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 10181, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  25%|██▍       | 50/201 [32:44<2:13:39, 53.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 9174, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  25%|██▌       | 51/201 [33:45<2:18:32, 55.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 8614, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  26%|██▋       | 53/201 [34:50<1:54:04, 46.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 7656, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  27%|██▋       | 54/201 [35:51<2:03:59, 50.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 7763, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  27%|██▋       | 55/201 [36:52<2:10:42, 53.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 12568, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  29%|██▉       | 58/201 [38:45<1:52:38, 47.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 7095, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  30%|███       | 61/201 [39:55<1:24:29, 36.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 9717, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  31%|███       | 62/201 [40:57<1:41:12, 43.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 11608, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  31%|███▏      | 63/201 [41:58<1:52:29, 48.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on tokens per minute (TPM): Limit 6000, Requested 10345, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  38%|███▊      | 77/201 [51:05<1:49:38, 53.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 198492, Requested 6207. Please try again in 33m49.745s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  39%|███▉      | 78/201 [52:05<1:53:31, 55.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 198351, Requested 6796. Please try again in 37m3.276s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  39%|███▉      | 79/201 [53:06<1:55:54, 57.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 198210, Requested 7694. Please try again in 42m30.497s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  40%|███▉      | 80/201 [54:07<1:57:16, 58.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 198070, Requested 8513. Please try again in 47m23.472s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  40%|████      | 81/201 [55:08<1:57:46, 58.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 197929, Requested 2528. Please try again in 3m17.238999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  41%|████      | 82/201 [56:08<1:57:47, 59.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 197789, Requested 2901. Please try again in 4m57.819s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  41%|████▏     | 83/201 [57:09<1:57:32, 59.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 197649, Requested 4137. Please try again in 12m51.141s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  42%|████▏     | 84/201 [58:10<1:57:07, 60.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 197508, Requested 5365. Please try again in 20m40.88s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  42%|████▏     | 85/201 [59:10<1:56:32, 60.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 197367, Requested 6113. Please try again in 25m3.207s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  43%|████▎     | 86/201 [1:00:11<1:55:43, 60.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 197227, Requested 5275. Please try again in 18m0.569999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  43%|████▎     | 87/201 [1:01:12<1:54:50, 60.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 197087, Requested 6776. Please try again in 27m48.411s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  44%|████▍     | 88/201 [1:02:13<1:54:15, 60.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 196945, Requested 3907. Please try again in 6m7.827999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  45%|████▍     | 90/201 [1:03:25<1:33:09, 50.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 200171, Requested 4169. Please try again in 31m15.160999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  45%|████▌     | 91/201 [1:04:26<1:38:03, 53.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 200030, Requested 4228. Please try again in 30m39.761999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  46%|████▌     | 92/201 [1:05:27<1:41:08, 55.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 199891, Requested 4145. Please try again in 29m3.336s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  46%|████▋     | 93/201 [1:06:28<1:43:09, 57.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 199749, Requested 18111. Please try again in 2h8m35.277999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  47%|████▋     | 94/201 [1:07:30<1:44:34, 58.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 199606, Requested 24214. Please try again in 2h51m30.101999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  47%|████▋     | 95/201 [1:08:32<1:45:25, 59.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 199463, Requested 18554. Please try again in 2h9m42.963s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  48%|████▊     | 96/201 [1:09:48<1:53:22, 64.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 199285, Requested 6099. Please try again in 38m45.693s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:  48%|████▊     | 97/201 [1:10:50<1:50:31, 63.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Summarization and Knowledge Extraction II: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01jb6awgzkfy7bjrz3wgh93byw` on : Limit 200000, Used 199143, Requested 3382. Please try again in 18m10.522s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during Summarization and Knowledge Extraction IV: 1 validation error for Summary\n",
      "bankruptcy_level\n",
      "  Input should be a valid dictionary or instance of BankruptcyLevel [type=model_type, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(os.listdir(bankrupt_files_path), total=len(os.listdir(bankrupt_files_path)), desc=\"Processing JSON files\"):\n",
    "    with open(os.path.join(bankrupt_files_path, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        # change the above to readline() to read the first main chunk of the text to avoid 413 error\n",
    "        # for 429 error code try to wait or do stuff in batches like first 20, then wait for 15 mins then 20 and so on...\n",
    "\n",
    "        try:\n",
    "            result = summary_extractor.extract_summary_and_knowledge(text, bankrupt_files_output)\n",
    "            os.makedirs(bankrupt_files_output, exist_ok=True)\n",
    "            output_file = os.path.join(bankrupt_files_output, f\"{file}.json\")\n",
    "            with(open(output_file, 'w')) as f:\n",
    "                json.dump(result.model_dump(), f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Summarization and Knowledge Extraction IV: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rewriting Valid JSON: 100%|██████████| 50/50 [00:00<00:00, 167.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# parsing the json output and saving it back to the file\n",
    "result_files = os.listdir(bankrupt_files_output)\n",
    "\n",
    "for file in tqdm(result_files, total=len(result_files), desc=\"Rewriting Valid JSON\"):\n",
    "    extracted_text_path = os.path.join(bankrupt_files_output, file)\n",
    "    with open(extracted_text_path, 'r', encoding='utf-8') as f:\n",
    "        extracted_text = json.load(f)\n",
    "        try:\n",
    "            start = extracted_text['content'].find('{')\n",
    "            end = extracted_text['content'].rfind('}')\n",
    "            if start==-1 or end == -1:\n",
    "                raise ValueError(\"No JSON object found in output\")\n",
    "            json_str  = extracted_text['content'][start: end+1]\n",
    "            json_str = json.loads(json_str)\n",
    "            with open(extracted_text_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_str, f, indent=4)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse JSON from output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run these as chunks to avoid error code 429 (request limit exceeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Healthy Files:  11%|█         | 30/280 [30:41<4:15:48, 61.39s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(os.listdir(healthy_files_path)[:30], total=len(os.listdir(healthy_files_path)), desc=\"Processing Healthy Files\"):\n",
    "    with open(os.path.join(healthy_files_path, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.readline()\n",
    "\n",
    "        try:\n",
    "            result = summary_extractor.extract_summary_and_knowledge(text, healthy_files_output)\n",
    "            os.makedirs(healthy_files_output, exist_ok=True)\n",
    "            output_file = os.path.join(healthy_files_output, f\"{file}.json\")\n",
    "            with(open(output_file, 'w')) as f:\n",
    "                json.dump(result.model_dump(), f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Summarization and Knowledge Extraction V: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(os.listdir(healthy_files_path)[31:60], total=len(os.listdir(healthy_files_path)[31:60]), desc=\"Processing Healthy Files\"):\n",
    "    with open(os.path.join(healthy_files_path, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.readline()\n",
    "\n",
    "        try:\n",
    "            result = summary_extractor.extract_summary_and_knowledge(text, healthy_files_output)\n",
    "            os.makedirs(healthy_files_output, exist_ok=True)\n",
    "            output_file = os.path.join(healthy_files_output, f\"{file}.json\")\n",
    "            with(open(output_file, 'w')) as f:\n",
    "                json.dump(result.model_dump(), f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Summarization and Knowledge Extraction V: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(healthy_files_path)[61:90]:\n",
    "    with open(os.path.join(healthy_files_path, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.readline()\n",
    "\n",
    "        try:\n",
    "            result = summary_extractor.extract_summary_and_knowledge(text, healthy_files_output)\n",
    "            os.makedirs(healthy_files_output, exist_ok=True)\n",
    "            output_file = os.path.join(healthy_files_output, f\"{file}.json\")\n",
    "            with(open(output_file, 'w')) as f:\n",
    "                json.dump(result.model_dump(), f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Summarization and Knowledge Extraction V: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(healthy_files_path)[91:120]:\n",
    "    with open(os.path.join(healthy_files_path, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.readline()\n",
    "\n",
    "        try:\n",
    "            result = summary_extractor.extract_summary_and_knowledge(text, healthy_files_output)\n",
    "            os.makedirs(healthy_files_output, exist_ok=True)\n",
    "            output_file = os.path.join(healthy_files_output, f\"{file}.json\")\n",
    "            with(open(output_file, 'w')) as f:\n",
    "                json.dump(result.model_dump(), f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Summarization and Knowledge Extraction V: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(healthy_files_path)[121:150]:\n",
    "    with open(os.path.join(healthy_files_path, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.readline()\n",
    "\n",
    "        try:\n",
    "            result = summary_extractor.extract_summary_and_knowledge(text, healthy_files_output)\n",
    "            os.makedirs(healthy_files_output, exist_ok=True)\n",
    "            output_file = os.path.join(healthy_files_output, f\"{file}.json\")\n",
    "            with(open(output_file, 'w')) as f:\n",
    "                json.dump(result.model_dump(), f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Summarization and Knowledge Extraction V: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rewriting Valid JSON: 100%|██████████| 50/50 [00:00<00:00, 140.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# parsing the json output and saving it back to the file\n",
    "result_files = os.listdir(healthy_files_output)\n",
    "\n",
    "for file in tqdm(result_files, total=len(result_files), desc=\"Rewriting Valid JSON\"):\n",
    "    extracted_text_path = os.path.join(healthy_files_output, file)\n",
    "    with open(extracted_text_path, 'r', encoding='utf-8') as f:\n",
    "        extracted_text = json.load(f)\n",
    "        try:\n",
    "            start = extracted_text['content'].find('{')\n",
    "            end = extracted_text['content'].rfind('}')\n",
    "            if start==-1 or end == -1:\n",
    "                raise ValueError(\"No JSON object found in output\")\n",
    "            json_str  = extracted_text['content'][start: end+1]\n",
    "            json_str = json.loads(json_str)\n",
    "            with open(extracted_text_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_str, f, indent=4)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse JSON from output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Intermediate json file example:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"content\": \"```\\n{\\n  \\\"summary\\\": \\\"ABG Shipyard Limited experienced a challenging financial year in 2012-2013, with the global economy and shipbuilding industry facing significant downturns. The company's financial status was affected by the decline in new shipbuilding orders and the subsequent discontinuation of the Shipbuilding Subsidy Scheme. However, the Indian government has been supportive of the industry and has taken various initiatives to improve the efficiency and productivity of domestic shipbuilding companies. The company's future prospects remain bright, with opportunities for growth in the maritime business and potential for development of the shipping sector.\\\",\\n  \\\"bankruptcy_level\\\": {\\\"lvl\\\": \\\"0\\\"},\\n  \\\"entities\\\": [\\n    {\\\"entity\\\": \\\"ABG Shipyard Limited\\\", \\\"type\\\": \\\"COMPANY\\\"},\\n    {\\\"entity\\\": \\\"Nisar & Kumar Chartered Accountants\\\", \\\"type\\\": \\\"COMPANY\\\"},\\n    {\\\"entity\\\": \\\"M. N. Ahmed\\\", \\\"type\\\": \\\"PERSON\\\"},\\n    {\\\"entity\\\": \\\"F. R. No. 107117W\\\", \\\"type\\\": \\\"PRODUCT\\\"},\\n    {\\\"entity\\\": \\\"Shipyards Association of India (SAI)\\\", \\\"type\\\": \\\"COMPANY\\\"},\\n    {\\\"entity\\\": \\\"Associate Chambers of Commerce (ASSOCHAM)\\\", \\\"type\\\": \\\"COMPANY\\\"},\\n    {\\\"entity\\\": \\\"Reserve Bank of India\\\", \\\"type\\\": \\\"COMPANY\\\"},\\n    {\\\"entity\\\": \\\"Indian Government\\\", \\\"type\\\": \\\"COMPANY\\\"},\\n    {\\\"entity\\\": \\\"Union budget 2013-14\\\", \\\"type\\\": \\\"EVENT\\\"}\\n  ],\\n  \\\"relations\\\": [\\n    {\\\"source\\\": \\\"ABG Shipyard Limited\\\", \\\"target\\\": \\\"M. N. Ahmed\\\", \\\"relation\\\": \\\"EMPLOYS\\\"},\\n    {\\\"source\\\": \\\"ABG Shipyard Limited\\\", \\\"target\\\": \\\"Shipyards Association of India (SAI)\\\", \\\"relation\\\": \\\"PARTICIPATES_IN\\\"},\\n    {\\\"source\\\": \\\"ABG Shipyard Limited\\\", \\\"target\\\": \\\"Associate Chambers of Commerce (ASSOCHAM)\\\", \\\"relation\\\": \\\"PARTICIPATES_IN\\\"},\\n    {\\\"source\\\": \\\"ABG Shipyard Limited\\\", \\\"target\\\": \\\"Reserve Bank of India\\\", \\\"relation\\\": \\\"PARTICIPATES_IN\\\"},\\n    {\\\"source\\\": \\\"ABG Shipyard Limited\\\", \\\"target\\\": \\\"Indian Government\\\", \\\"relation\\\": \\\"PARTICIPATES_IN\\\"},\\n    {\\\"source\\\": \\\"ABG Shipyard Limited\\\", \\\"target\\\": \\\"Union budget 2013-14\\\", \\\"relation\\\": \\\"PARTICIPATES_IN\\\"},\\n    {\\\"source\\\": \\\"Nisar & Kumar Chartered Accountants\\\", \\\"target\\\": \\\"ABG Shipyard Limited\\\", \\\"relation\\\": \\\"MENTIONS\\\"},\\n    {\\\"source\\\": \\\"M. N. Ahmed\\\", \\\"target\\\": \\\"ABG Shipyard Limited\\\", \\\"relation\\\": \\\"MENTIONS\\\"},\\n    {\\\"source\\\": \\\"Shipyards Association of India (SAI)\\\", \\\"target\\\": \\\"ABG Shipyard Limited\\\", \\\"relation\\\": \\\"MENTIONS\\\"},\\n    {\\\"source\\\": \\\"Associate Chambers of Commerce (ASSOCHAM)\\\", \\\"target\\\": \\\"ABG Shipyard Limited\\\", \\\"relation\\\": \\\"MENTIONS\\\"},\\n    {\\\"source\\\": \\\"Reserve Bank of India\\\", \\\"target\\\": \\\"ABG Shipyard Limited\\\", \\\"relation\\\": \\\"MENTIONS\\\"},\\n    {\\\"source\\\": \\\"Indian Government\\\", \\\"target\\\": \\\"ABG Shipyard Limited\\\", \\\"relation\\\": \\\"MENTIONS\\\"},\\n    {\\\"source\\\": \\\"Union budget 2013-14\\\", \\\"target\\\": \\\"ABG Shipyard Limited\\\", \\\"relation\\\": \\\"MENTIONS\\\"}\\n  ]\\n}\\n```\",\n",
    "    \"additional_kwargs\": {},\n",
    "    \"response_metadata\": {\n",
    "        \"token_usage\": {\n",
    "            \"completion_tokens\": 723,\n",
    "            \"prompt_tokens\": 2500,\n",
    "            \"total_tokens\": 3223,\n",
    "            \"completion_time\": 2.892,\n",
    "            \"prompt_time\": 0.490955819,\n",
    "            \"queue_time\": 0.02852289899999999,\n",
    "            \"total_time\": 3.382955819\n",
    "        },\n",
    "        \"model_name\": \"llama-3.1-70b-versatile\",\n",
    "        \"system_fingerprint\": \"fp_b3ae7e594e\",\n",
    "        \"finish_reason\": \"stop\",\n",
    "        \"logprobs\": null\n",
    "    },\n",
    "    \"type\": \"ai\",\n",
    "    \"name\": null,\n",
    "    \"id\": \"run-b98d84d2-7a6c-421f-8ea5-0a2ce32ef718-0\",\n",
    "    \"example\": false,\n",
    "    \"tool_calls\": [],\n",
    "    \"invalid_tool_calls\": [],\n",
    "    \"usage_metadata\": {\n",
    "        \"input_tokens\": 2500,\n",
    "        \"output_tokens\": 723,\n",
    "        \"total_tokens\": 3223\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final sample json file result.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"summary\": \"ABG Shipyard Limited is a company that operates in the shipbuilding industry. The company's financial year 2012-13 was challenging due to the global economic downturn. The Indian economy experienced a low growth rate, and the shipbuilding industry was affected by the decline in new shipbuilding orders. However, the company remains optimistic about its future prospects, citing the favorable demographics and the directional commitment towards liberalization in the Indian economy. The company's bankruptcy level is -1, indicating that it is a healthy company.\",\n",
    "    \"bankruptcy_level\": {\n",
    "        \"lvl\": \"-1\"\n",
    "    },\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"entity\": \"ABG Shipyard Limited\",\n",
    "            \"type\": \"COMPANY\"\n",
    "        },\n",
    "        {\n",
    "            \"entity\": \"Nisar & Kumar Chartered Accountants\",\n",
    "            \"type\": \"COMPANY\"\n",
    "        },\n",
    "        {\n",
    "            \"entity\": \"M. N. Ahmed\",\n",
    "            \"type\": \"PERSON\"\n",
    "        },\n",
    "        {\n",
    "            \"entity\": \"Indian Government\",\n",
    "            \"type\": \"COMPANY\"\n",
    "        },\n",
    "        {\n",
    "            \"entity\": \"Reserve Bank of India\",\n",
    "            \"type\": \"COMPANY\"\n",
    "        },\n",
    "        {\n",
    "            \"entity\": \"Associate Chambers of Commerce\",\n",
    "            \"type\": \"COMPANY\"\n",
    "        },\n",
    "        {\n",
    "            \"entity\": \"Shipyards Association of India\",\n",
    "            \"type\": \"COMPANY\"\n",
    "        }\n",
    "    ],\n",
    "    \"relations\": [\n",
    "        {\n",
    "            \"source\": \"ABG Shipyard Limited\",\n",
    "            \"target\": \"M. N. Ahmed\",\n",
    "            \"relation\": \"EMPLOYS\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"ABG Shipyard Limited\",\n",
    "            \"target\": \"Indian Government\",\n",
    "            \"relation\": \"PARTICIPATES_IN\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"Reserve Bank of India\",\n",
    "            \"target\": \"ABG Shipyard Limited\",\n",
    "            \"relation\": \"PARTICIPATES_IN\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"Associate Chambers of Commerce\",\n",
    "            \"target\": \"ABG Shipyard Limited\",\n",
    "            \"relation\": \"MENTIONS\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"Shipyards Association of India\",\n",
    "            \"target\": \"ABG Shipyard Limited\",\n",
    "            \"relation\": \"MENTIONS\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
